import torch
from torch.distributions.categorical import Categorical
from train_agent import mars_agent_gradient_oneshot
# from train_agent import mars_agent_gradient, mars_agent_gradient_oneshot


def mars_selector_oneshot(out, num_batch, actor_temp, actor_spec, critic, actor_temp_optimizer, actor_spec_optimizer, critic_optimizer,
                      msnn, xb, yb, criterion, epoch, gam=0.96, r_gam=10, train_type='train'):
    features = out.clone()  # B, ch, 1, t    ch:32, t:33
    B, C, E, T = features.shape
    features = torch.cat([features, torch.zeros(B, 1, E, T, device=features.device)], dim=1)
    C += 1
    mask = torch.ones_like(features)  # Mask generated by the agent module

    state_temp_list = []
    state_temp_prime_list = []
    state_spec_list = []
    state_spec_prime_list = []
    action_probs_temp_list = []
    action_probs_spec_list = []
    action_probs_temp_prime_list = []
    action_probs_spec_prime_list = []
    action_temp_list = []
    action_spec_list = []

    for t in range(C - 1):  # t = 1,...,T'
        with torch.no_grad():
            # To avoid zero-division.
            # temporal action
            mask_temp = mask.clone()  # (B, C, 1, T)
            mask_spec = mask.clone()  # (B, C, 1, T)
            features *= mask

            # GAP
            if t == 0:
                GAP_temp = torch.zeros_like(features[:, :, 0, t]).squeeze()
                GAP_spec = torch.zeros_like(features[:, t, 0, :]).squeeze()
            else:
                m_temp = mask_temp[:, :, 0, :t].sum(dim=-1).squeeze()
                m_spec = mask_spec[:, :t, 0, :].sum(dim=-2).squeeze()
                m_temp[m_temp==0] = 1
                m_spec[m_spec==0] = 1
                GAP_temp = features[:, :, 0, :t].sum(dim=-1) / m_temp  # expect B, C
                GAP_spec = features[:, :t, 0, :].sum(dim=-2) / m_spec  # expect B, T

            cur_temp_feature = features[:, :, 0, t].squeeze()  # expect B, C
            cur_spec_feature = features[:, t, 0, :].squeeze()  # expect B, T

            state_temp = torch.cat([GAP_temp, cur_temp_feature], dim=-1)  # B, 2C
            state_spec = torch.cat([GAP_spec, cur_spec_feature], dim=-1)  # B, 2T

            # Get action, a_t.
            action_probs_temp = actor_temp(state_temp)  # B, 2

            action_temp = Categorical(probs=action_probs_temp).sample().unsqueeze(-1).repeat(1, T)
            action_probs_spec = actor_spec(state_spec)  # B, 2
            action_spec = Categorical(probs=action_probs_spec).sample().unsqueeze(-1).repeat(1, C)

            # action = np.tile(tf.random.categorical(action_probs, 1).numpy(), 112)  # (5, 112)
            mask_temp[:, :, 0, t] = action_temp
            mask_spec[:, t, 0, :] = action_spec
            mask = mask_temp * mask_spec

            m_temp = mask_temp[:, :, 0, :t+1].sum(dim=-1).squeeze()
            m_spec = mask_spec[:, :t+1, 0, :].sum(dim=-2).squeeze()
            m_temp[m_temp == 0] = 1
            m_spec[m_spec == 0] = 1

            GAP_temp_prime = features[:, :, 0, :t+1].sum(dim=-1) / m_temp  # expect B, C
            GAP_spec_prime = features[:, :t+1, 0, :].sum(dim=-2) / m_spec  # expect B, T

            cur_temp_feature_prime = features[:, :, 0, t].squeeze()  # expect B, C
            cur_spec_feature_prime = features[:, t, 0, :].squeeze()  # expect B, T

            state_temp_prime = torch.cat([GAP_temp_prime, cur_temp_feature_prime], dim=-1)  # B, 2C
            state_spec_prime = torch.cat([GAP_spec_prime, cur_spec_feature_prime], dim=-1)  # B, 2T

            action_probs_temp_prime = actor_temp(state_temp_prime)  # B, 2
            action_probs_spec_prime = actor_spec(state_spec_prime)  # B, 2

            state_temp_list.append(state_temp)
            state_temp_prime_list.append(state_temp_prime)
            state_spec_list.append(state_spec)
            state_spec_prime_list.append(state_spec_prime)
            action_probs_temp_list.append(action_probs_temp)
            action_probs_spec_list.append(action_probs_spec)
            action_probs_temp_prime_list.append(action_probs_temp_prime)
            action_probs_spec_prime_list.append(action_probs_spec_prime)
            action_temp_list.append(action_temp)
            action_spec_list.append(action_spec)

    out *= mask[:, :-1, :, :]


    if train_type == 'train':
        critic_loss, actor_temp_loss, actor_spec_loss = \
            mars_agent_gradient_oneshot(msnn, actor_temp, actor_spec, critic, xb,
                                        out.clone().detach(), yb,
                                        state_temp_list, state_temp_prime_list,
                                        state_spec_list, state_spec_prime_list,
                                        action_probs_temp_list, action_probs_spec_list,
                                        action_probs_temp_prime_list, action_probs_spec_prime_list,
                                        action_temp_list, action_spec_list,
                                        critic_optimizer, actor_temp_optimizer,
                                        actor_spec_optimizer,
                                        criterion, epoch, gam=gam, r_gam=r_gam)

        actor_temp_optimizer.zero_grad()
        actor_temp_loss.backward(retain_graph=True)
        actor_temp_optimizer.step()
        actor_spec_optimizer.zero_grad()
        actor_spec_loss.backward(retain_graph=True)
        actor_spec_optimizer.step()
        critic_optimizer.zero_grad()
        critic_loss.backward()
        critic_optimizer.step()

    return mask[:, :-1, :, :]







